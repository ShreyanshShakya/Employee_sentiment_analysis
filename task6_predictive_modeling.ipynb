{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c4bccc",
   "metadata": {},
   "source": [
    "# Task 6: Predictive Modeling\n",
    "\n",
    "**Objective**: Develop a linear regression model to analyze and predict sentiment trends.\n",
    "\n",
    "**Features**: Message frequency, average message length, word count, previous month score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dafe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled dataset\n",
    "df = pd.read_csv('data/test_labeled.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "print(f\"Loaded {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c596c7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating features that may influence sentiment scores:\n",
    "- **message_frequency**: Number of messages per month\n",
    "- **avg_message_length**: Average character count\n",
    "- **avg_word_count**: Average words per message\n",
    "- **prev_month_score**: Previous month's sentiment score (lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features per employee per month\n",
    "features = df.groupby(['employee', 'year_month']).agg({\n",
    "    'sentiment_score': 'sum',\n",
    "    'message_length': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'full_message': 'count'\n",
    "}).rename(columns={\n",
    "    'full_message': 'message_frequency',\n",
    "    'sentiment_score': 'monthly_score'\n",
    "}).reset_index()\n",
    "\n",
    "# Add lag feature (previous month score)\n",
    "features = features.sort_values(['employee', 'year_month'])\n",
    "features['prev_month_score'] = features.groupby('employee')['monthly_score'].shift(1)\n",
    "features = features.dropna()\n",
    "\n",
    "print(f\"Feature matrix: {len(features)} samples\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62e6d3",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = ['message_frequency', 'message_length', 'word_count', 'prev_month_score']\n",
    "X = features[feature_cols]\n",
    "y = features['monthly_score']\n",
    "\n",
    "print(\"Feature Statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Testing: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"✅ Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d5cb9",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LINEAR REGRESSION MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTRAINING SET:\")\n",
    "print(f\"  R² Score: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  MSE: {mean_squared_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "print(f\"  R² Score: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  MSE: {mean_squared_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1abc2b",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE COEFFICIENTS (Importance)\")\n",
    "print(\"=\" * 60)\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "for _, row in coef_df.iterrows():\n",
    "    direction = \"↑\" if row['Coefficient'] > 0 else \"↓\"\n",
    "    print(f\"  {row['Feature']:<20}: {row['Coefficient']:>8.4f} {direction}\")\n",
    "print(f\"  {'Intercept':<20}: {model.intercept_:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb44b18",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00380de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, color='steelblue', s=30)\n",
    "min_val, max_val = min(y_test.min(), y_pred_test.min()), max(y_test.max(), y_pred_test.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Score', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Score', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Residuals Distribution\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].hist(residuals, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Feature Importance\n",
    "colors = ['green' if c > 0 else 'red' for c in coef_df['Coefficient']]\n",
    "axes[2].barh(coef_df['Feature'], coef_df['Coefficient'], color=colors)\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[2].set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/model_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✅ Saved: model_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee6220",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a9533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "The linear regression model reveals key insights about sentiment predictors:\n",
    "\n",
    "1. PREVIOUS MONTH SCORE: The strongest predictor. Employees tend to maintain\n",
    "   similar sentiment patterns month-over-month.\n",
    "\n",
    "2. MESSAGE FREQUENCY: {'Positive' if model.coef_[0] > 0 else 'Negative'} correlation \n",
    "   with sentiment score. {'More active employees tend to be more positive.' if model.coef_[0] > 0 else 'Higher activity may indicate stress.'}\n",
    "\n",
    "3. MESSAGE LENGTH: {'Longer' if model.coef_[1] > 0 else 'Shorter'} messages \n",
    "   correlate with {'higher' if model.coef_[1] > 0 else 'lower'} sentiment scores.\n",
    "\n",
    "4. WORD COUNT: Similar pattern to message length.\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "- R² = {r2_score(y_test, y_pred_test):.3f} → Explains {r2_score(y_test, y_pred_test)*100:.1f}% of variance\n",
    "- MAE = {mean_absolute_error(y_test, y_pred_test):.2f} → Average prediction error\n",
    "\n",
    "The model captures general trends but individual predictions vary due to \n",
    "the inherent complexity of human sentiment.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model results\n",
    "results = {\n",
    "    'r2_train': r2_score(y_train, y_pred_train),\n",
    "    'r2_test': r2_score(y_test, y_pred_test),\n",
    "    'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "    'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "    'coefficients': dict(zip(feature_cols, model.coef_)),\n",
    "    'intercept': model.intercept_\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('data/model_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Model results saved to data/model_results.json\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
